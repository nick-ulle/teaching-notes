{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "Basic web scraping workflow:\n",
    "\n",
    "1. Download pages (use a GET request) -- [`requests`][] and [`requests_cache`][]\n",
    "2. Parse pages to extract text -- [`lxml.html`][] or [`bs4`][]\n",
    "3. Clean up extracted text -- string methods or [`re`][] or [`pandas`][]\n",
    "4. Store cleaned results -- [`pandas`][], [`sqlite3`][], [`pymongo`][], or ...\n",
    "5. Analyze results -- [`pandas`][], and ...\n",
    "\n",
    "Other than the packages involved, this workflow is the same regardless of the language you're using.\n",
    "\n",
    "[`requests`]: http://docs.python-requests.org/en/master/\n",
    "[`requests_cache`]: https://requests-cache.readthedocs.io/en/latest/\n",
    "[`lxml.html`]: http://lxml.de/lxmlhtml.html\n",
    "[`bs4`]: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "[`re`]: https://docs.python.org/2/library/re.html\n",
    "[`pandas`]: http://pandas.pydata.org/pandas-docs/stable/\n",
    "[`sqlite3`]: https://docs.python.org/2/library/sqlite3.html\n",
    "[`pymongo`]: http://api.mongodb.com/python/current/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Parsing HTML\n",
    "\n",
    "You can use `lxml.html` or `bs4` to parse HTML. Choose one for the entire scrape, since the two packages are not compatible with each other.\n",
    "\n",
    "An advantage of `lxml.html` is that it supports both XPath and CSS Selectors. It's also faster than BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = u\"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<title>This is the Title!</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "<p>This is a paragraph!</p>\n",
    "<p>This is another paragraph!</p>\n",
    "<span>This is a span. ❤️  </span>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "import lxml.html as lx\n",
    "\n",
    "html = lx.fromstring(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a paragraph!\n",
      "This is another paragraph!\n"
     ]
    }
   ],
   "source": [
    "# Extract a tag with XPath\n",
    "print html.xpath(\"/html/body/p\")[0].text_content()\n",
    "\n",
    "# In XPath, \"//\" means \"at this level or anywhere below\" \"/.../\"\n",
    "print html.xpath(\"//p\")[1].text_content()\n",
    "\n",
    "# http://www.topswagcode.com/xpath/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a paragraph!\n",
      "This is another paragraph!\n"
     ]
    }
   ],
   "source": [
    "# Extract a tag with CSS Selectors\n",
    "print html.cssselect(\"html > body > p\")[0].text_content()\n",
    "\n",
    "# In CSS, \" \" means \"at this level or anywhere below\"\n",
    "print html.cssselect(\"p\")[1].text_content()\n",
    "\n",
    "# http://flukeout.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Cleaning Up\n",
    "\n",
    "By this point, there shouldn't be any HTML tags left in the text you've extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a paragraph!\\nThis is another paragraph!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list = [x.text_content().strip() for x in html.cssselect(\"p\")]\n",
    "\"\\n\".join(p_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unicode\n",
    "\n",
    "Since computers only understand numbers, text is encoded by assigning a number to each symbol. There are many different text encodings, and until the 1980s there was no global standard. The United States used to use the [ASCII encoding](https://en.wikipedia.org/wiki/ASCII), which only covers English characters.\n",
    "\n",
    "[Unicode](http://unicode.org/) is a global standard for text encoding. Unicode [includes symbols](http://unicode.org/charts/) for nearly all languages in use today, as well as emoji and many ancient languages (such as Egyptian hieroglyphs).\n",
    "\n",
    "In Python 2, Unicode strings are prefixed with a `u` before the first quote. For example, `u\"This is Unicode\"` is a Unicode string. All of the built-in string methods will work on Unicode strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'This is a span. \\u2764\\ufe0f  '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html.cssselect(\"span\")[0].text_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you display a Unicode string without printing, special Unicode characters will be represented by `\\uXXXX` where `XXXX` is the number assigned to the character in [base 16](https://en.wikipedia.org/wiki/Hexadecimal). Base 16 numbers are sometimes prefixed with `0x` to distinguish them from decimal numbers.\n",
    "\n",
    "For example, the heart emoji ❤ is assigned to `10084`, which is `0x2764`. You can write the heart emoji in a Unicode string as `\\u2764`. The pink heart emoji ❤️ consists of a heart emoji character `\\u2764` followed by an invisible \"text presentation\" character `\\ufe0f`. See [Wikipedia's emoji article](https://en.wikipedia.org/wiki/Emoji#Emoji_versus_text_presentation) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'This is a span. \\u2764\\ufe0f'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html.cssselect(\"span\")[0].text_content().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many Unicode characters are larger than 1 byte, which can cause problems for some regular expression engines.\n",
    "\n",
    "Python 2's `re` module works with Unicode strings. The module [has a flag](https://docs.python.org/2/library/re.html#re.UNICODE) `re.UNICODE` to inform the regex engine when you want to use Unicode strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "Basic NLP workflow:\n",
    "\n",
    "1. __Tokenize__ -- split text into words\n",
    "2. __Denoise__ (optional) -- remove stop words, convert words to lemmas, correct spelling, ...\n",
    "3. __Vectorize__ -- compute term frequencies, tf-idfs, or some other statistic\n",
    "4. __Analyze__\n",
    "\n",
    "The _smoothed term frequency-inverse document frequency_ (smoothed tf-idf), for a token $t$ and document $d$, is given by\n",
    "$$\n",
    "\\operatorname{tf-idf}(t, d) = \\operatorname{tf}(t, d) \\cdot \\log \\left( \\frac{N}{1 + n_t} \\right)\n",
    "$$\n",
    "where $N$ is the total number of documents and $n_t$ is the number of documents that contain $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Set up the Reuters corpus.\n",
    "reuters = [nltk.corpus.reuters.raw(i) for i in nltk.corpus.reuters.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.word_tokenize()\n",
    "#stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#vectorizer = TfidfVectorizer(tokenizer = , stop_words = \"english\", smooth_idf = True, norm = None)\n",
    "#tfs = vectorizer.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics with Python\n",
    "\n",
    "`scikit-learn` is a popular and mature package for statistics, with particular focus on machine learning methods. The [documentation](http://scikit-learn.org/stable/documentation.html) has both a user guide and an API reference.\n",
    "\n",
    "The user guide discusses the intuition (and sometimes mathematics) behind the supported statistical methods, with examples. The API reference is a detailed description of each function in the package.\n",
    "\n",
    "Since `scikit-learn`'s user guide is not intended to teach statistics, you might also want to look at other resources to learn more about statistical methods. Both of these books are free online:\n",
    "\n",
    "* [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "* [Elements of Statistical Learning](https://statweb.stanford.edu/~tibs/ElemStatLearn/)\n",
    "\n",
    "ISL is a gentle introduction to statistical methods, while ESL is a more thorough reference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
